# SLURMCTLD
SlurmUser={{ slurm_username }}
{% for host in groups["slurm-master"] %}
SlurmctldHost={{ host }}
{% endfor %}
SlurmctldPort=6817
StateSaveLocation=/var/lib/slurm/slurmctld

# SLURMD
SlurmdPort=6818
SlurmdSpoolDir=/var/lib/slurm/slurmd
ReturnToService=1

# ACCOUNTING
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost={{ groups["slurm-master"][0] }}
AccountingStorageUser={{ slurm_db_username }}
AccountingStoragePass=/var/run/munge/munge.socket.2
{% if slurm_accounting_mode == 'strict' %}
AccountingStorageEnforce=associations,limits,qos
{% endif %}
ClusterName={{ slurm_cluster_name }}

# SCHEDULING
SelectType=select/linear
SelectTypeParameters=CR_Memory
SchedulerType=sched/backfill
SchedulerParameters=kill_invalid_depend,nohold_on_prolog_fail
PreemptType=preempt/partition_prio
PreemptMode=CANCEL
PreemptExemptTime=1:00:00

# TASK SETTINGS
TaskPlugin=affinity,cgroup
TaskPluginParam=Sched,Autobind=None
PropagateResourceLimitsExcept=MEMLOCK
PrologFlags=alloc,contain
Prolog={{ _sysconf_dir }}/prolog.sh
Epilog={{ _sysconf_dir }}/epilog.sh

# MPI
MpiDefault=pmix
TmpFS=/tmp/slurm

# MISC
AuthType=auth/munge
DebugFlags=CPU_Bind
MailProg=/usr/bin/s-nail
RebootProgram=/sbin/reboot

# PARTITIONS
PartitionName=batch Nodes=ALL Default=YES DefMemPerCPU=0 State=UP OverSubscribe=EXCLUSIVE

# NODES
{% for node_name in groups['slurm-node'] %}
{% set memory =  hostvars[node_name]["ansible_local"]["memory"] -%}
{% set cpu_topology =  hostvars[node_name]["ansible_local"]["topology"]["cpu_topology"] -%}
{% set gpu_topology =  hostvars[node_name]["ansible_local"]["topology"]["gpu_topology"] -%}
    NodeName={{ node_name }}{{ " " -}}
    CPUs={{ cpu_topology.logical_cpus|int }}{{ " " -}}
    Sockets={{ cpu_topology.sockets|int }}{{ " " -}}
    CoresPerSocket={{ cpu_topology.cores_per_socket|int }}{{ " " -}}
    ThreadsPerCore={{ cpu_topology.threads_per_core }}{{ " " -}}
    Procs={{ cpu_topology.sockets|int * cpu_topology.cores_per_socket|int }}{{ " " -}}
    RealMemory={{ memory.total_mb|int }}
{% endfor %}
