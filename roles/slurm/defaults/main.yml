epel_baseurl: "https://download.fedoraproject.org/pub/epel/$releasever/$basearch/"
epel_gpgkey: "https://epel.mirror.constant.com//RPM-GPG-KEY-EPEL-{{ ansible_distribution_major_version }}"

slurm_version: 20.02.1
slurm_src_url: "https://download.schedmd.com/slurm/slurm-{{ slurm_version }}.tar.bz2"
slurm_build_dir: /tmp/slurm-build
slurm_config_dir: /etc/slurm
slurm_install_prefix: /usr/local
slurm_configure:  './configure --prefix={{ slurm_install_prefix }} --disable-dependency-tracking --disable-debug --disable-x11 --enable-really-no-cray --enable-salloc-kill-cmd --with-hdf5=no --sysconfdir={{ slurm_config_dir }} --enable-pam --with-pam_dir={{ slurm_pam_lib_dir }} --without-shared-libslurm --without-rpath'

slurm_hpl: no
slurm_include_hwloc: yes
slurm_include_pmix: yes
slurm_include_openblas: no

slurm_cluster_name: deepops
slurm_username: slurm
slurm_password: ReplaceWithASecurePasswordInTheVault
slurm_user_uid: 22078
slurm_user_home: /local/slurm

slurm_allow_ssh_user: []

slurm_db_username: slurm
slurm_db_password: AlsoReplaceWithASecurePasswordInTheVault

is_controller: no
is_compute: no

# Controls when a DOWN node will be returned to service.
# See "man slurm.conf" for more details.
slurm_return_to_service: 1

# Sets: GresTypes=gpu
# Sets: Gres=gpu:{{ gpu_topology|count }}
slurm_manage_gpus: true

# Controls the ability of the partition to execute more than one job at a time on each resource (node, socket or core depending upon the value of SelectTypeParameters). If resources are to be over-subscribed, avoiding memory over-subscription is very important. SelectTypeParameters should be configured to treat memory as a consumable resource and the --mem option should be used for job allocations. Sharing of resources is typically useful only when using gang scheduling (PreemptMode=suspend,gang). Possible values for OverSubscribe are "EXCLUSIVE", "FORCE", "YES", and "NO". Note that a value of "YES" or "FORCE" can negatively impact performance for systems with many thousands of running jobs. The default value is "NO". For more information see the following web pages: 
# Sets partition OverSubscribe
# To avoid sharing nodes, set to "EXCLUSIVE"
slurm_oversubscribe: "NO"

# Maximum run time limit for jobs. Format is minutes, minutes:seconds, hours:minutes:seconds, days-hours, days-hours:minutes, days-hours:minutes:seconds or "UNLIMITED". Time resolution is one minute and second values are rounded up to the next minute. This limit does not apply to jobs executed by SlurmUser or user root.
# Sets: MaxTime={{ slurm_max_job_timelimit }}
# Default: INFINITE
# slurm_max_job_timelimit: INFINITE

# Run time limit used for jobs that don't specify a value. If not set then MaxTime will be used. Format is the same as for MaxTime.
# Sets: DefaultTime={{ slurm_default_job_timelimit }}
# Default: <not included>
# slurm_default_job_timelimit: 

# Use the provided prolog/epilog
slurm_enable_prolog_epilog: true

# Useful to delete old files (e.g. from previous versions of deepops)
slurm_clear_old_prolog_epilog: false

# Disable clearing of shared memory when user login shell exits
slurm_fix_shm: true

slurm_force_rebuild: no

pmix_version: 3.1.5
pmix_src_url: "https://github.com/openpmix/openpmix/releases/download/v{{ pmix_version }}/pmix-{{ pmix_version }}.tar.bz2"
pmix_build_dir: /tmp/pmix-build
pmix_install_prefix: "{{ slurm_install_prefix }}"
pmix_configure: "./configure --prefix={{ pmix_install_prefix }} --with-hwloc={{ hwloc_install_prefix }} --with-munge"
pmix_force_rebuild: no

hwloc_version: 2.2.0
hwloc_tag: "v{{ hwloc_version.split('.')[0] }}.{{ hwloc_version.split('.')[1] }}"
hwloc_src_url: "https://download.open-mpi.org/release/hwloc/{{ hwloc_tag }}/hwloc-{{ hwloc_version }}.tar.gz"
hwloc_build_dir: /tmp/hwloc-build
hwloc_install_prefix: "{{ slurm_install_prefix }}"
hwloc_configure: "./configure --prefix={{ hwloc_install_prefix }} --enable-static --disable-cairo --disable-libxml2 --enable-netloc --enable-plugins"
hwloc_force_rebuild: no

openblas_version: 0.3.9
openblas_src_url: "https://github.com/xianyi/OpenBLAS/archive/v{{ openblas_version }}.tar.gz"
openblas_build_dir: /tmp/openblas-build
openblas_install_prefix: "{{ slurm_install_prefix }}"
openblas_force_rebuild: no
