---
- hosts: slurm-master
  become: yes
  roles:
    - ood-ansible
  vars:
    user: "{{ ansible_env.SUDO_USER | default(ansible_env.USER) }}"
    ood_source_version: "v1.7.6"
    cluster_config_dir: /etc/ood/config/clusters.d
    ood_desktops_dir: /etc/ood/config/apps/bc_desktop
    ood_install_apps:
      bc_osc_codeserver:
        repo: https://github.com/OSC/bc_osc_codeserver.git
        dest: /var/www/ood/apps/sys
        version: gtc
    ood_slurm_config: |
      ---
      v2:
        metadata:
          title: "NVIDIA DeepOps Cluster"
        login:
          host: "{{ ansible_fqdn }}"
        job:
          adapter: "slurm"
          cluster: "deepops"
          bin: "/usr/bin"
          conf: "/etc/slurm/slurm.conf"
        batch_connect:
          basic:
            script_wrapper: |
              %s
          vnc:
            script_wrapper: |
              export PATH="/opt/TurboVNC/bin:$PATH"
              export WEBSOCKIFY_CMD="/usr/bin/websockify"
              %s
    ood_desktop_config: |
      ---
      title: "DeepOps Desktop"
      cluster: "deepops"
      attributes:
        desktop: "xfce"
        bc_queue: null
        bc_account: null
    ood_frontpage_config: |
      pun_custom_env:
        OOD_DASHBOARD_TITLE: "NVIDIA DeepOps Cluster"
        OOD_BRAND_BG_COLOR: "#76B900"
        OOD_BRAND_LINK_ACTIVE_BG_COLOR: "#333"
        OOD_NAVBAR_TYPE: "inverse"
  tasks:
    - name: create clusters dir
      file:
        path: "{{ cluster_config_dir }}"
        state: directory
      tags:
        - configure
    - name: configure slurm
      blockinfile:
        path: "{{ cluster_config_dir }}/deepops.yml"
        block: |
          {{ ood_slurm_config }}
        create: yes
        marker: "# {mark} ANSIBLE MANAGED BLOCK insertion 1"
      tags:
        - configure
    - name: create desktop app dir
      file:
        path: "{{ ood_desktops_dir }}"
        state: directory
      tags:
        - configure
    - name: configure desktop
      blockinfile:
        path: "{{ ood_desktops_dir }}/deepops.yml"
        block: |
          {{ ood_desktop_config }}
        create: yes
      tags:
        - configure
    - name: configure front page
      blockinfile:
        path: "/etc/ood/config/nginx_stage.yml"
        block: |
          {{ ood_frontpage_config }}
        create: yes
      tags:
        - configure
    - name: install package dependency for htpasswd module
      package:
        name: python-passlib
        state: present
      tags:
        - configure
    - name: create .htpasswd entries
      htpasswd:
        path: /etc/apache2/.htpasswd
        name: "{{ user }}"
        password: 'deepops'
        create: yes
      tags:
        - configure
    - name: install required apache module
      apache2_module:
        state: present
        name: proxy_wstunnel
      register: mod_install
      tags:
        - configure
    - name: restart apache after module install
      systemd:
        service: apache2
        state: restarted
      when: mod_install.changed
      tags:
        - configure
    - name: create ssh key pair for default user
      openssh_keypair:
        path: "/home/{{ user }}/.ssh/id_rsa"
        owner: "{{ user }}"
      tags:
        - configure
    - name: get ssh pub key
      slurp:
        src: "/home/{{ user }}/.ssh/id_rsa.pub"
      register: user_pub_key
      tags:
        - configure
    - name: set ssh authorized key
      authorized_key:
        user: "{{ user }}"
        state: present
        key: "{{ user_pub_key['content'] | b64decode }}"
      tags:
        - configure

- hosts: slurm-master
  become: yes
  vars:
    ood_codeserver_app_dir: /var/www/ood/apps/sys/bc_osc_codeserver
    ood_codeserver_app_form: |
      ---
      cluster: "deepops"
      form:
        - bc_num_gpus
        - bc_num_hours
        - working_dir
      attributes:
        working_dir:
          label: "Working Directory"
          data-filepicker: true
          data-target-file-type: dirs  # Valid values are: files, dirs, or both
          readonly: false
          help: "Select your project directory; defaults to $HOME"
        bc_num_gpus:
          label: "Number of GPUs"
          value: 1
    ood_codeserver_app_manifest: |
      ---
      name: VS Code Server
      category: Interactive Apps
      subcategory: Servers
      role: batch_connect
      description: |
        This app will launch a [VS Code] instance using [Code Server] on a GPU node

        [VS Code]: https://code.visualstudio.com/
        [Code Server]: https://coder.com/
    ood_codeserver_app_submit: |
      ---
      batch_connect:
        template: "basic"
      script:
        native:
          - "--gpus=<%= bc_num_gpus.blank? ? 1 : bc_num_gpus.to_i %>"
          - "--cpus-per-gpu=4"
          - "--mem-per-gpu=16G"
    ood_codeserver_app_script: |
      #!/usr/bin/env bash
      <%-
          # Ensure that code-server always starts up in either a user defined directory or the home directory
          working_dir = Pathname.new(context.working_dir)
          if ! working_dir.exist? || working_dir.to_s.empty?
              working_dir = Pathname.new(ENV['HOME'])
          elsif working_dir.file?
              working_dir = working_dir.parent
          end
      %>

      # Setup environment
      CODE_SERVER="${CODE_SERVER:-/usr/local/bin/code-server}"
      CODE_SERVER_DATAROOT="$HOME/.local/share/bc_osc_codeserver"


      # Ensure a dataroot for Code Server
      mkdir -p "$CODE_SERVER_DATAROOT/extensions"

      # Expose the password to the server
      export PASSWORD="$password"

      # VSCode complains that system git is too old
      #module load git

      #
      # Start Code Server
      #
      echo $CODE_SERVER

      # An arbitrary path...
      $CODE_SERVER \
          --auth=password \
          --port="$port" \
          --disable-telemetry \
          --extra-extensions-dir="$CODE_SERVER_DATAROOT/extensions" \
          --user-data-dir="$CODE_SERVER_DATAROOT" \
          "<%= working_dir.to_s %>"
  tasks:
    - name: remove old config
      file:
        path: "{{ ood_codeserver_app_dir }}/{{ item }}"
        state: absent
      with_items:
        - form.yml
        - manifest.yml
        - submit.yml.erb
        - template/script.sh.erb
      tags:
        - configure
    - name: configure vs code app form
      blockinfile:
        path: "{{ ood_codeserver_app_dir }}/form.yml"
        block: |
          {{ ood_codeserver_app_form }}
        create: yes
      tags:
        - configure
    - name: configure vs code app manifest
      blockinfile:
        path: "{{ ood_codeserver_app_dir }}/manifest.yml"
        block: |
          {{ ood_codeserver_app_manifest }}
        create: yes
      tags:
        - configure
    - name: configure vs code app submit
      blockinfile:
        path: "{{ ood_codeserver_app_dir }}/submit.yml.erb"
        block: |
          {{ ood_codeserver_app_submit }}
        create: yes
      tags:
        - configure
    - name: configure vs code app script
      blockinfile:
        path: "{{ ood_codeserver_app_dir }}/template/script.sh.erb"
        block: |
          {{ ood_codeserver_app_script }}
        create: yes
        mode: 0755
      tags:
        - configure

- hosts: slurm-node
  become: yes
  tasks:
    - name: install turbovnc
      apt:
        deb: https://downloads.sourceforge.net/project/turbovnc/2.2.3/turbovnc_2.2.3_amd64.deb
      tags:
        - vnc
    - name: install codeserver
      unarchive:
        src: https://github.com/cdr/code-server/releases/download/2.1698/code-server2.1698-vsc1.41.1-linux-x86_64.tar.gz
        remote_src: yes
        dest: /usr/local/bin
        exclude:
          - README.md
          - LICENSE.txt
          - ThirdPartyNotices.txt
        creates: /usr/local/bin/code-server
        mode: 0755
        extra_opts:
          - --strip-components=1
      tags:
        - codeserver

# Set up LinuxHost adapter 
- hosts: slurm-master
  become: yes
  vars:
    cluster_config_dir: /etc/ood/config/clusters.d
    singularity_image_dir: /mnt/shared/singularity-images
    ood_linuxhost_adapter_script: |
      #!/bin/bash
      set -e

      PAM_UID=$(id -u "${PAM_USER}")

      if [ "${PAM_SERVICE}" = "sshd" -a "${PAM_UID}" -ge 1000 ]; then
              /usr/bin/systemctl set-property "user-${PAM_UID}.slice" \
                      MemoryAccounting=true MemoryLimit=64G \
                      CPUAccounting=true \
                      CPUQuota=700%
      fi
    ood_linuxhost_adapter_config: |
      ---
      v2:
        metadata:
          title: "NVIDIA DeepOps Cluster"
        login:
          host: "localhost"
        job:
          adapter: "linux_host"
          submit_host: "localhost"
          ssh_hosts:
            - localhost
          site_timeout: 7200
          debug: true
          singularity_bin: /usr/bin/singularity
          singularity_bindpath: /etc,/media,/mnt,/opt,/run,/srv,/usr,/var,/users
          singularity_image: {{ singularity_image_dir }}/ubuntu-18.04.simg
          # Enabling strict host checking may cause the adapter to fail if the user's known_hosts does not have all the roundrobin hosts
          strict_host_checking: false
  tasks:
    - name: configure PAM
      lineinfile:
        path: "/etc/pam.d/systemd-user-slice"
        line: "session     required      pam_exec.so type=open_session /etc/security/limits.sh"
        mode: 0644
        create: yes
      tags:
        - linuxhost-adapter
    - name: configure script
      blockinfile:
        path: "/etc/security/limits.sh"
        block: |
          {{ ood_linuxhost_adapter_script }}
        mode: 0755
        create: yes
      tags:
        - linuxhost-adapter
    - name: configure linuxhost-adapter
      blockinfile:
        path: "{{ cluster_config_dir }}/linuxhost.yml"
        block: |
          {{ ood_linuxhost_adapter_config }}
        create: yes
        marker: "# {mark} ANSIBLE MANAGED BLOCK insertion 2"
      tags:
        - linuxhost-adapter
    - name: create singularity image directory
      file:
        path: "{{ singularity_image_dir }}"
        state: directory
      tags:
        - linuxhost-adapter
    - name: pull singularity image
      command: singularity pull --name ubuntu-18.04.simg docker://ubuntu:18.04
      args:
        chdir: "{{ singularity_image_dir }}"
        creates: "{{ singularity_image_dir }}/ubuntu-18.04.simg"
      tags:
        - linuxhost-adapter
